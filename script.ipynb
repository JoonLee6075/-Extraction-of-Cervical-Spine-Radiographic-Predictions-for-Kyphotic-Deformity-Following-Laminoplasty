{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05ff24b-17f7-4260-8e52-4ec5188dbd83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CHECK WHAT GPU UR USING\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22de58bf-2f81-4b1b-aa01-987ed7fac770",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install these critical packages for this project\n",
    "!pip install SimpleITK\n",
    "!pip install keras-preprocessing\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ef4176a-e4bb-43f8-b5c3-b44a52d9f345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the packages u need\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv3D, BatchNormalization, Dropout, MaxPooling3D, UpSampling3D, Conv3DTranspose, Concatenate, Activation, Add\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6403d9-8af3-4f03-b1c2-db9495150ce5",
   "metadata": {},
   "source": [
    "DEFINE PREPROCESSING CODE. were taking class 4 and turning it to 0 so we have only 4 classes (0[the background],1,2, and 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca136e74-5e7a-460a-b598-3904822f20d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def preprocess_image_mask(image_file, mask_file):\n",
    "    # Load NRRD files (image and mask)\n",
    "    image = sitk.ReadImage(image_file.decode())\n",
    "    mask = sitk.ReadImage(mask_file.decode())\n",
    "\n",
    "    # Convert to NumPy arrays\n",
    "    image = sitk.GetArrayFromImage(image)\n",
    "    mask = sitk.GetArrayFromImage(mask)\n",
    "\n",
    "    # Scale and convert image to float16\n",
    "    img_min = np.min(image)\n",
    "    img_max = np.max(image)\n",
    "    if img_max > img_min:\n",
    "        image = (image - img_min) / (img_max - img_min)\n",
    "    image = image.astype(np.float16)  # Ensure image is float16\n",
    "    image = np.expand_dims(image, axis=-1)\n",
    "\n",
    "    # Convert class 4 to 0\n",
    "    mask[mask == 4] = 0\n",
    "\n",
    "    # Convert all classes > 0 to 1 for binary classification\n",
    "    mask[mask > 0] = 1\n",
    "\n",
    "    # Convert the mask to float16\n",
    "    mask = mask.astype(np.float16)\n",
    "\n",
    "    # Expand the mask dimensions to match the image dimensions\n",
    "    mask = np.expand_dims(mask, axis=-1)\n",
    "\n",
    "    return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96168d66-335c-4636-b5c6-28e075179a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Parameters\n",
    "batch_size = 20 \n",
    "n_classes = 1\n",
    "seed = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bd86bdd-10a1-47e6-be14-01e9319334a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "def create_dataset(image_dir, mask_dir, batch_size):\n",
    "    image_files = sorted(glob.glob(image_dir))\n",
    "    mask_files = sorted(glob.glob(mask_dir))\n",
    "\n",
    "    # Ensure both lists have the same length\n",
    "    assert len(image_files) == len(mask_files), \"Image and mask files lists must be the same length.\"\n",
    "\n",
    "    # Generate indices and shuffle them\n",
    "    indices = np.arange(len(image_files))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # Use the shuffled indices to reorder the file lists\n",
    "    shuffled_image_files = [image_files[i] for i in indices]\n",
    "    shuffled_mask_files = [mask_files[i] for i in indices]\n",
    "\n",
    "    # Create a TensorFlow dataset from the shuffled file names\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((shuffled_image_files, shuffled_mask_files))\n",
    "\n",
    "    def _parse_function(image_file, mask_file):\n",
    "        # Load image and mask as float16\n",
    "        image, mask = tf.numpy_function(preprocess_image_mask, [image_file, mask_file], [tf.float16, tf.float16])\n",
    "        image.set_shape([2000, 2000, 1])  # Adjusted for 2D shape\n",
    "        mask.set_shape([2000, 2000, 1])   # Adjusted for 2D shape without one-hot encoding (binary classification)\n",
    "        return image, mask\n",
    "\n",
    "    dataset = dataset.map(_parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.repeat()\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3077415b-4ebe-46bd-8fd6-9d3b9a75443a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_frequencies(binary_mask_batch):\n",
    "    # Flatten the mask batch to calculate the frequencies across the entire batch\n",
    "    mask_flattened = tf.reshape(binary_mask_batch, [-1])\n",
    "    \n",
    "    # Convert mask to int32 for tf.math.bincount\n",
    "    mask_flattened = tf.cast(mask_flattened, tf.int32)\n",
    "    \n",
    "    # Count the occurrences of each class (0 and 1)\n",
    "    class_counts = tf.math.bincount(mask_flattened, minlength=2, maxlength=2)\n",
    "    \n",
    "    # Cast class_counts to float32 for consistent data type during division\n",
    "    class_counts = tf.cast(class_counts, tf.float32)\n",
    "    \n",
    "    # Normalize the class counts to get the frequencies\n",
    "    total_pixels = tf.reduce_sum(class_counts)\n",
    "    class_frequencies = class_counts / total_pixels\n",
    "    \n",
    "    return class_frequencies.numpy()\n",
    "\n",
    "# Define paths and batch size\n",
    "# If workspace local file path is different after \"workspace\" folder, update the file path below\n",
    "train_img_dir = \"/workspace/trainxray22/*.nrrd\"\n",
    "train_mask_dir = \"/workspace/traingt2/*.nrrd\"\n",
    "\n",
    "# Create the dataset\n",
    "dataset = create_dataset(train_img_dir, train_mask_dir, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af340d9-bc2d-4a95-b2d6-55d17ddb1b4f",
   "metadata": {},
   "source": [
    "DEFINE YOUR TRAIN AND VALIDATION COMMANDS and print the shape of ur data so all looks good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "541c3153-c258-4051-b46f-ab836e60fb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 12075\n",
      "Number of validation samples: 2710\n",
      "Training Image Batch Shape: (20, 2000, 2000, 1)\n",
      "Training Mask Batch Shape: (20, 2000, 2000, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "\n",
    "def calculate_num_samples(image_dir, mask_dir):\n",
    "    image_files = sorted(glob.glob(image_dir))\n",
    "    mask_files = sorted(glob.glob(mask_dir))\n",
    "    \n",
    "    if len(image_files) != len(mask_files):\n",
    "        raise ValueError(\"Number of image files and mask files do not match.\")\n",
    "    \n",
    "    file_pairs = list(zip(image_files, mask_files))\n",
    "    return len(file_pairs)\n",
    "\n",
    "val_img_dir = \"/workspace/valxray/*.nrrd\"\n",
    "val_mask_dir = \"/workspace/valgt/*.nrrd\"\n",
    "\n",
    "# Calculate number of samples\n",
    "num_train_samples = calculate_num_samples(train_img_dir, train_mask_dir)\n",
    "num_val_samples = calculate_num_samples(val_img_dir, val_mask_dir)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = create_dataset(train_img_dir, train_mask_dir, batch_size)\n",
    "val_dataset = create_dataset(val_img_dir, val_mask_dir, batch_size)\n",
    "\n",
    "# Print sample counts, uncomment for debugging sample counts\n",
    "# print(f\"Number of training samples: {num_train_samples}\")\n",
    "# print(f\"Number of validation samples: {num_val_samples}\")\n",
    "\n",
    "# Inspect the first batch\n",
    "for img_batch, mask_batch in train_dataset.take(1):\n",
    "    print(\"Training Image Batch Shape:\", img_batch.shape)\n",
    "    print(\"Training Mask Batch Shape:\", mask_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f221c3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f1cfded-af75-4612-bef0-3786632d270d",
   "metadata": {},
   "source": [
    "PLOT YOUR XRAYS AND GTs SIDE BY SIDE TO CONFIRM ALL IS GOOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918ecdf5-d883-4d22-b941-d3b016ea45fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Get the first batch of images and masks\n",
    "for img_batch, mask_batch in train_dataset.take(1):\n",
    "    # Convert to numpy arrays for easier handling\n",
    "    img_batch = img_batch.numpy()\n",
    "    mask_batch = mask_batch.numpy()\n",
    "    \n",
    "    # Plot each image and its corresponding ground truth\n",
    "    for i in range(batch_size):\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "        # Display the image in grayscale (assuming the image has a single channel)\n",
    "        axs[0].imshow(np.squeeze(img_batch[i]), cmap=\"gray\")\n",
    "        axs[0].set_title(f\"Scan {i+1}\")\n",
    "        axs[0].axis(\"off\")\n",
    "\n",
    "        # Display the binary ground truth mask\n",
    "        axs[1].imshow(np.squeeze(mask_batch[i]), cmap=\"gray\")\n",
    "        axs[1].set_title(f\"Ground Truth {i+1}\")\n",
    "        axs[1].axis(\"off\")\n",
    "\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56835a47-ee9f-4bd9-a8d8-d1025749d580",
   "metadata": {},
   "source": [
    "ALL LOOKS GOOD. NOW, STARTING BUILDING UR MODEL, DEFINE YOUR LOSS FUNCTION, METRICS, and TRAIN!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0809b01-673f-4c44-9e1f-88e91a195578",
   "metadata": {},
   "source": [
    "LOSS FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a828c27-efce-4db0-8fb0-c9c5662f083e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def tversky_loss(y_true, y_pred, alpha=0.23, beta=0.99, smooth=1):\n",
    "    y_true = K.flatten(y_true)\n",
    "    y_pred = K.flatten(y_pred)\n",
    "    true_pos = K.sum(y_true * y_pred)\n",
    "    false_neg = K.sum(y_true * (1 - y_pred))\n",
    "    false_pos = K.sum((1 - y_true) * y_pred)\n",
    "    tversky_index = (true_pos + smooth) / (true_pos + alpha * false_pos + beta * false_neg + smooth)\n",
    "    return 1 - tversky_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee4b28e-2daa-4387-a68a-4256138d8652",
   "metadata": {},
   "source": [
    "METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ebb253a-fb63-4b93-a619-77a62ac07009",
   "metadata": {},
   "outputs": [],
   "source": [
    "#METRICS\n",
    "# IoU Score Metrics\n",
    "def iou_score(y_true, y_pred):\n",
    "    smoothing_factor = 1\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=[1,2,3])\n",
    "    union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3]) - intersection\n",
    "    iou = K.mean((intersection + smoothing_factor) / (union + smoothing_factor), axis=0)\n",
    "    return iou\n",
    "\n",
    "def dice_coefficient(y_true, y_pred):\n",
    "    smoothing_factor = 1\n",
    "    flat_y_true = K.flatten(y_true)\n",
    "    flat_y_pred = K.flatten(y_pred)\n",
    "    \n",
    "    intersection = K.sum(flat_y_true * flat_y_pred)\n",
    "    union = K.sum(flat_y_true) + K.sum(flat_y_pred)\n",
    "    \n",
    "    dice = (2. * intersection + smoothing_factor) / (union + smoothing_factor)\n",
    "    return dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db40436d-9e70-4808-80fb-062e6ee2b908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Activation, BatchNormalization, add, multiply, UpSampling2D, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "import keras.backend as K\n",
    "\n",
    "def expend_as(tensor, rep, name):\n",
    "    my_repeat = Lambda(lambda x, repnum: K.repeat_elements(x, repnum, axis=3), arguments={'repnum': rep}, name='psi_up' + name)(tensor)\n",
    "    return my_repeat\n",
    "\n",
    "# Define the 2D attention gating block\n",
    "def AttnGatingBlock(x, g, inter_shape, name):\n",
    "    shape_x = K.int_shape(x)\n",
    "    shape_g = K.int_shape(g)\n",
    "\n",
    "    theta_x = Conv2D(inter_shape, (2, 2), strides=(2, 2), padding='same', name='xl' + name)(x)\n",
    "    shape_theta_x = K.int_shape(theta_x)\n",
    "\n",
    "    phi_g = Conv2D(inter_shape, (1, 1), padding='same')(g)\n",
    "    upsample_g = Conv2DTranspose(inter_shape, (3, 3),\n",
    "                                 strides=(shape_theta_x[1] // shape_g[1], shape_theta_x[2] // shape_g[2]),\n",
    "                                 padding='same', name='g_up' + name)(phi_g)\n",
    "\n",
    "    concat_xg = add([upsample_g, theta_x])\n",
    "    act_xg = Activation('relu')(concat_xg)\n",
    "\n",
    "    psi = Conv2D(1, (1, 1), padding='same', name='psi' + name)(act_xg)\n",
    "    sigmoid_xg = Activation('sigmoid')(psi)\n",
    "\n",
    "    shape_sigmoid = K.int_shape(sigmoid_xg)\n",
    "    upsample_psi = UpSampling2D(size=(shape_x[1] // shape_sigmoid[1], shape_x[2] // shape_sigmoid[2]))(sigmoid_xg)\n",
    "\n",
    "    upsample_psi = Conv2D(shape_x[3], (1, 1), padding='same')(upsample_psi)\n",
    "\n",
    "    y = multiply([upsample_psi, x], name='q_attn' + name)\n",
    "\n",
    "    result = Conv2D(shape_x[3], (1, 1), padding='same', name='q_attn_conv' + name)(y)\n",
    "    result_bn = BatchNormalization(name='q_attn_bn' + name)(result)\n",
    "    return result_bn\n",
    "\n",
    "# Define the 2D U-Net convolutional block with residual connections\n",
    "def UnetConv2D(input, outdim, is_batchnorm, num_conv_layers, name):\n",
    "    x = input\n",
    "    for i in range(num_conv_layers):\n",
    "        x = Conv2D(outdim, (3, 3), strides=(1, 1), kernel_initializer='he_normal', padding=\"same\", name=f'{name}_{i+1}')(x)\n",
    "        if is_batchnorm:\n",
    "            x = BatchNormalization(name=f'{name}_{i+1}_bn')(x)\n",
    "        x = Activation('relu', name=f'{name}_{i+1}_relu')(x)\n",
    "    return x\n",
    "\n",
    "# Define the 2D gating signal\n",
    "def UnetGatingSignal(input, is_batchnorm, name):\n",
    "    shape = K.int_shape(input)\n",
    "    x = Conv2D(shape[3], (1, 1), strides=(1, 1), padding=\"same\", kernel_initializer='he_normal', name=name + '_conv')(input)\n",
    "    if is_batchnorm:\n",
    "        x = BatchNormalization(name=name + '_bn')(x)\n",
    "    x = Activation('relu', name=name + '_act')(x)\n",
    "    return x\n",
    "\n",
    "# Define the 2D U-Net with attention gates and residual connections\n",
    "def attn_unet(opt, input_size, lossfxn, metrics, n_classes):\n",
    "    inputs = Input(shape=input_size)\n",
    "    \n",
    "    # Contracting path\n",
    "    conv1 = UnetConv2D(inputs, 16, is_batchnorm=True, num_conv_layers=2, name='conv1')\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    \n",
    "    conv2 = UnetConv2D(pool1, 32, is_batchnorm=True, num_conv_layers=3, name='conv2')\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = UnetConv2D(pool2, 64, is_batchnorm=True, num_conv_layers=3, name='conv3')\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = UnetConv2D(pool3, 128, is_batchnorm=True, num_conv_layers=3, name='conv4')\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    center = UnetConv2D(pool4, 256, is_batchnorm=True, num_conv_layers=3, name='center')\n",
    "    \n",
    "    # Expanding path with attention gates\n",
    "    g1 = UnetGatingSignal(center, is_batchnorm=True, name='g1')\n",
    "    up1 = concatenate([Conv2DTranspose(128, (3, 3), strides=(2, 2), padding='same', kernel_initializer='he_normal')(center), conv4], name='up1')\n",
    "    \n",
    "    g2 = UnetGatingSignal(up1, is_batchnorm=True, name='g2')\n",
    "    attn2 = AttnGatingBlock(conv3, g2, 128, '_2')\n",
    "    up2 = concatenate([Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same', kernel_initializer='he_normal')(up1), attn2], name='up2')\n",
    "\n",
    "    g3 = UnetGatingSignal(up2, is_batchnorm=True, name='g3')\n",
    "    attn3 = AttnGatingBlock(conv2, g3, 64, '_3')\n",
    "    up3 = concatenate([Conv2DTranspose(32, (3, 3), strides=(2, 2), padding='same', kernel_initializer='he_normal')(up2), attn3], name='up3')\n",
    "\n",
    "    g4 = UnetGatingSignal(up3, is_batchnorm=True, name='g4')\n",
    "    attn4 = AttnGatingBlock(conv1, g4, 32, '_4')\n",
    "    up4 = concatenate([Conv2DTranspose(16, (3, 3), strides=(2, 2), padding='same', kernel_initializer='he_normal')(up3), attn4], name='up4')\n",
    "    \n",
    "    # Output layer with softmax activation for multiclass classification\n",
    "    out = Conv2D(n_classes, (1, 1), activation='sigmoid', kernel_initializer='he_normal', name='final')(up4)\n",
    "    \n",
    "    model = Model(inputs=[inputs], outputs=[out])\n",
    "    model.compile(optimizer=opt, loss=lossfxn, metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ca6fcc-6d2e-40d4-bc30-d1366a22c360",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    gpu_info = !nvidia-smi\n",
    "    gpu_info = '\\n'.join(gpu_info)\n",
    "    print(gpu_info)\n",
    "\n",
    "def print_model_device_placement(model):\n",
    "    # Check if GPU is available\n",
    "    physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if not physical_devices:\n",
    "        print(\"GPU is not available\")\n",
    "        return\n",
    "    \n",
    "    print(\"GPU is available\")\n",
    "\n",
    "# Initialize strategy for distributed training\n",
    "strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.NcclAllReduce())\n",
    "print(\"Number of devices: {}\".format(strategy.num_replicas_in_sync))\n",
    "\n",
    "with strategy.scope():\n",
    "    initial_learning_rate = 0.0001  # Set your desired initial learning rate\n",
    "    \n",
    "    optim = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate)\n",
    "    \n",
    "    # Create the model using Focal Tversky Loss\n",
    "    model = attn_unet(\n",
    "        optim, \n",
    "        (2000, 2000, 1), \n",
    "        tversky_loss,  # Pass tversky_loss directly\n",
    "        [iou_score, dice_coefficient], \n",
    "        n_classes\n",
    "    )\n",
    "    \n",
    "    # Print model summary\n",
    "    model.summary()\n",
    "    \n",
    "    # Define the ReduceLROnPlateau callback\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=4,\n",
    "        verbose=1,\n",
    "        min_lr=1e-6\n",
    "    )\n",
    "    \n",
    "    # Define ModelCheckpoint to save the best model\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        'best_model.h5',\n",
    "        monitor='val_loss',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode='min'\n",
    "    )\n",
    "    \n",
    "    # Define TensorBoard callback\n",
    "    tensorboard = TensorBoard(\n",
    "        log_dir='./logs',\n",
    "        histogram_freq=1,\n",
    "        write_graph=True,\n",
    "        write_images=True\n",
    "    )\n",
    "    \n",
    "    # Print GPU utilization and model device placement\n",
    "    print_gpu_utilization()\n",
    "    print_model_device_placement(model)\n",
    "\n",
    "    # Add TensorBoard to the list of callbacks\n",
    "    callbacks = [reduce_lr, checkpoint, tensorboard]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84b0811-60f3-42cb-a753-a0ad0c07cd07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 40\n",
    "\n",
    "# Define ModelCheckpoint callback\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath='Models/Natalia3E{epoch:02d}.keras',  # Path where to save the model, including epoch\n",
    "    save_best_only=False,  # Save the model after every epoch\n",
    "    monitor='val_loss',  # Still monitor validation loss to log improvements\n",
    "    mode='min',  # The smaller the monitored quantity, the better\n",
    "    verbose=1  # Log a message after every epoch\n",
    ")\n",
    "\n",
    "# Define the whole list of callbacks\n",
    "callbacks = [reduce_lr, checkpoint, tensorboard, model_checkpoint_callback]\n",
    "\n",
    "# Fit the model using the custom generator for training data\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=num_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=num_val_samples // batch_size,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aa8038-f527-462d-81d9-f9168ff26a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate the list of epochs\n",
    "epochs = range(1, len(history.history['iou_score']) + 1)\n",
    "\n",
    "# Plot Training and Validation IoU\n",
    "iou = history.history['iou_score']\n",
    "val_iou = history.history['val_iou_score']\n",
    "plt.plot(epochs, iou, 'y', label='Training IoU')\n",
    "plt.plot(epochs, val_iou, 'r', label='Validation IoU')\n",
    "plt.title('Training and validation IoU')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('IoU')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot Training and Validation Dice Coefficient\n",
    "dice = history.history['dice_coefficient']\n",
    "val_dice = history.history['val_dice_coefficient']\n",
    "plt.plot(epochs, dice, 'y', label='Training Dice')\n",
    "plt.plot(epochs, val_dice, 'r', label='Validation Dice')\n",
    "plt.title('Training and validation Dice Coefficient')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Dice Coefficient')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74116be4-2fac-4b79-a442-d4b1053181fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model for future use\n",
    "model.save('/workspace/FinalBoss40.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77de945-90df-4b2a-be39-3429d5411cb0",
   "metadata": {},
   "source": [
    "Load in the model if you have just opened this kernel without training it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4d47c5e-fe9e-43f8-9aaf-18f2a3ca23c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Define the dice_coefficient function\n",
    "def dice_coefficient(y_true, y_pred):\n",
    "    smoothing_factor = 1\n",
    "    flat_y_true = K.flatten(y_true)\n",
    "    flat_y_pred = K.flatten(y_pred)\n",
    "    \n",
    "    intersection = K.sum(flat_y_true * flat_y_pred)\n",
    "    union = K.sum(flat_y_true) + K.sum(flat_y_pred)\n",
    "    \n",
    "    dice = (2. * intersection + smoothing_factor) / (union + smoothing_factor)\n",
    "    return dice\n",
    "\n",
    "# Other custom functions or losses, like iou_score\n",
    "def iou_score(y_true, y_pred):\n",
    "    smoothing_factor = 1\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=[1, 2, 3])\n",
    "    union = K.sum(y_true, axis=[1, 2, 3]) + K.sum(y_pred, axis=[1, 2, 3]) - intersection\n",
    "    iou = K.mean((intersection + smoothing_factor) / (union + smoothing_factor), axis=0)\n",
    "    return iou\n",
    "\n",
    "from keras.saving import register_keras_serializable\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "@register_keras_serializable()\n",
    "def tversky_loss(y_true, y_pred, alpha=0.23, beta=0.99, smooth=1):\n",
    "    y_true = K.flatten(y_true)\n",
    "    y_pred = K.flatten(y_pred)\n",
    "    true_pos = K.sum(y_true * y_pred)\n",
    "    false_neg = K.sum(y_true * (1 - y_pred))\n",
    "    false_pos = K.sum((1 - y_true) * y_pred)\n",
    "    tversky_index = (true_pos + smooth) / (true_pos + alpha * false_pos + beta * false_neg + smooth)\n",
    "    return 1 - tversky_index\n",
    "\n",
    "model = load_model(\n",
    "    '/workspace/FinalBoss40.keras',\n",
    "    custom_objects={\n",
    "        'dice_coefficient': dice_coefficient,\n",
    "        'iou_score': iou_score,\n",
    "        'tversky_loss': tversky_loss\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcf6174-599e-415b-9151-955774c15238",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1907c719-ef40-48ad-a7c4-e40f42edea84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the first and last dimensions if they exist (i.e., shape is (1, 2000, 2000, 1))\n",
    "binary_mask = mask_batch[0]\n",
    "\n",
    "# Squeeze the first dimension (if it's 1) and then squeeze the last dimension (if it's 1)\n",
    "if len(binary_mask.shape) == 4 and binary_mask.shape[0] == 1:\n",
    "    binary_mask = np.squeeze(binary_mask, axis=0)  # Remove the first dimension\n",
    "\n",
    "if len(binary_mask.shape) == 3 and binary_mask.shape[-1] == 1:\n",
    "    binary_mask = np.squeeze(binary_mask, axis=-1)  # Remove the last dimension\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21e4a69c-b11f-4139-9b6a-c013daf8e6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37282779-b8c2-484b-ab8c-4ac4d90194f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "def create_dataset(image_dir, mask_dir, batch_size):\n",
    "    # Get all image and mask file paths and sort them in order\n",
    "    image_files = sorted(glob.glob(image_dir))\n",
    "    mask_files = sorted(glob.glob(mask_dir))\n",
    "\n",
    "    # Ensure both lists have the same length\n",
    "    assert len(image_files) == len(mask_files), \"Image and mask files lists must be the same length.\"\n",
    "\n",
    "    # Create a TensorFlow dataset from the file names\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_files, mask_files))\n",
    "\n",
    "    def _parse_function(image_file, mask_file):\n",
    "        # Load image and mask as float16\n",
    "        image, mask = tf.numpy_function(preprocess_image_mask, [image_file, mask_file], [tf.float16, tf.float16])\n",
    "        image.set_shape([2000, 2000, 1])  # Adjusted for 2D shape\n",
    "        mask.set_shape([2000, 2000, 1])   # Adjusted for 2D shape without one-hot encoding (binary classification)\n",
    "        return image, mask\n",
    "\n",
    "    dataset = dataset.map(_parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.repeat()\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db4ad0a-e6ae-46fc-85e9-8558c13766bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e45a068-1675-4dc3-a852-c46d181c2b1a",
   "metadata": {},
   "source": [
    "DELETE ONCE READY TO SUBMIT TO TOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ab7ea5-248a-4c68-8e17-e51f2bd541e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2  # Using OpenCV for connected components\n",
    "import math\n",
    "\n",
    "# Function to calculate the number of samples in the test dataset\n",
    "def calculate_num_samples(image_dir, mask_dir):\n",
    "    file_pairs = list(zip(sorted(glob.glob(image_dir)), sorted(glob.glob(mask_dir))))\n",
    "    return len(file_pairs)\n",
    "\n",
    "# Directories for test set, update according to your local directory\n",
    "test_img_dir = \"/workspace/testxray/*.nrrd\"\n",
    "test_mask_dir = \"/workspace/testgt/*.nrrd\"\n",
    "\n",
    "# Calculate number of test samples\n",
    "num_test_samples = calculate_num_samples(test_img_dir, test_mask_dir)\n",
    "\n",
    "# Set batch size to 1\n",
    "batch_size = 1\n",
    "\n",
    "# Create test dataset\n",
    "test_dataset = create_dataset(test_img_dir, test_mask_dir, batch_size)\n",
    "\n",
    "# Print the number of test samples\n",
    "# print(f\"Number of test samples: {num_test_samples}\")\n",
    "\n",
    "# Function to calculate distances and angles (already provided)\n",
    "def get_left_right_corners(contour):\n",
    "    \"\"\"Get the left-most and right-most points of a contour.\"\"\"\n",
    "    left_corner = tuple(contour[contour[:, :, 0].argmin()][0])\n",
    "    right_corner = tuple(contour[contour[:, :, 0].argmax()][0])\n",
    "    return left_corner, right_corner\n",
    "\n",
    "def calculate_distances_and_angles(binary_mask):\n",
    "    \"\"\"\n",
    "    This function calculates C2-C7 angle, CGH-C7 SVA, and C2 and C7 slopes.\n",
    "    \"\"\"\n",
    "    # Ensure mask is binary\n",
    "    binary_mask = (binary_mask > 0).astype(np.uint8)\n",
    "\n",
    "    # Find connected components\n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(binary_mask, connectivity=8)\n",
    "    \n",
    "    # Ensure we have exactly 3 clusters (acoustic meatus, C2, and C7)\n",
    "    if num_labels != 4:  # We expect 4 labels (1 background + 3 objects)\n",
    "        raise ValueError(f\"Expected 3 segmented regions, but found {num_labels - 1}\")\n",
    "    \n",
    "    # Extract relevant clusters based on centroid positions (sorting vertically)\n",
    "    components = sorted([(i, centroids[i]) for i in range(1, num_labels)], key=lambda x: x[1][1])\n",
    "\n",
    "    # Assign the components\n",
    "    acoustic_meatus_label = components[0][0]\n",
    "    c2_label = components[1][0]\n",
    "    c7_label = components[2][0]\n",
    "\n",
    "    # Get the contours for each region\n",
    "    contours_acoustic_meatus, _ = cv2.findContours((labels == acoustic_meatus_label).astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours_c2, _ = cv2.findContours((labels == c2_label).astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours_c7, _ = cv2.findContours((labels == c7_label).astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # 1. C2-C7 Angle Calculation\n",
    "    c2_left_corner, c2_right_corner = get_left_right_corners(contours_c2[0])  # Posterior and anterior C2 corners\n",
    "    c7_left_corner, c7_right_corner = get_left_right_corners(contours_c7[0])  # Posterior and anterior C7 corners\n",
    "\n",
    "    # Slopes of C2 and C7 lines\n",
    "    c2_line_slope = (c2_right_corner[1] - c2_left_corner[1]) / (c2_right_corner[0] - c2_left_corner[0])\n",
    "    c7_line_slope = (c7_right_corner[1] - c7_left_corner[1]) / (c7_right_corner[0] - c7_left_corner[0])\n",
    "\n",
    "    # Perpendicular slopes (negative reciprocal)\n",
    "    c2_perpendicular_slope = -1 / c2_line_slope\n",
    "    c7_perpendicular_slope = -1 / c7_line_slope\n",
    "\n",
    "    # Angle between the perpendicular lines\n",
    "    tan_theta = abs((c7_perpendicular_slope - c2_perpendicular_slope) / (1 + c7_perpendicular_slope * c2_perpendicular_slope))\n",
    "    c2_c7_angle = math.degrees(math.atan(tan_theta))\n",
    "\n",
    "    results = {'c2_c7_angle': c2_c7_angle}\n",
    "\n",
    "    # 2. CGH-C7 SVA Calculation\n",
    "    acoustic_meatus_anterior = tuple(contours_acoustic_meatus[0][contours_acoustic_meatus[0][:, :, 0].argmax()][0])\n",
    "    c7_posterior_corner = tuple(contours_c7[0][contours_c7[0][:, :, 0].argmin()][0])\n",
    "\n",
    "    # Calculate the pixel distance\n",
    "    sva_pixels = np.abs(acoustic_meatus_anterior[0] - c7_posterior_corner[0])\n",
    "\n",
    "    # Convert pixel distance to millimeters using the median pixel spacing\n",
    "    pixel_spacing = 0.139  # mm\n",
    "    sva_mm = sva_pixels * pixel_spacing\n",
    "\n",
    "    results['cgh_c7_sva'] = sva_mm\n",
    "\n",
    "    # 3. C2 and C7 Slope Calculation\n",
    "    c2_slope = math.degrees(math.atan2(c2_right_corner[1] - c2_left_corner[1], c2_right_corner[0] - c2_left_corner[0]))\n",
    "    c7_slope = math.degrees(math.atan2(c7_right_corner[1] - c7_left_corner[1], c7_right_corner[0] - c7_left_corner[0]))\n",
    "\n",
    "    results['c2_slope'] = c2_slope\n",
    "    results['c7_slope'] = c7_slope\n",
    "\n",
    "    return results, contours_acoustic_meatus, contours_c2, contours_c7, c2_left_corner, c2_right_corner, c7_left_corner, c7_right_corner, c2_perpendicular_slope, c7_perpendicular_slope\n",
    "\n",
    "# Preprocess prediction to only include the top 3 largest clusters\n",
    "def preprocess_prediction(pred_mask):\n",
    "    # Perform connected component analysis using OpenCV\n",
    "    num_labels, labels = cv2.connectedComponents(pred_mask)\n",
    "\n",
    "    # List the areas of each component (excluding background label)\n",
    "    areas = [np.sum(labels == i) for i in range(1, num_labels)]  # Skip the background (label 0)\n",
    "\n",
    "    # Sort the components by area (size) and get the three largest\n",
    "    largest_labels = sorted(range(1, num_labels), key=lambda i: areas[i-1], reverse=True)[:3]\n",
    "\n",
    "    # If there are less than 3 clusters, print a message and return None\n",
    "    if len(largest_labels) < 3:\n",
    "        print(f\"Failed to identify 3 anatomical landmarks in this prediction.\")\n",
    "        return None\n",
    "\n",
    "    # Create a mask for the three largest components\n",
    "    largest_mask = np.zeros_like(pred_mask)\n",
    "    for label in largest_labels:\n",
    "        largest_mask[labels == label] = 1\n",
    "\n",
    "    return largest_mask\n",
    "\n",
    "# Plotting measurements on the mask\n",
    "def plot_all(test_image, binary_mask, contours_acoustic_meatus, contours_c2, contours_c7, \n",
    "             c2_left_corner, c2_right_corner, c7_left_corner, c7_right_corner, \n",
    "             c2_perpendicular_slope, c7_perpendicular_slope, pred_results):\n",
    "    # Create a figure with 3 subplots in a row and set the figure size to 2000x2000\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # Set the axis limits to 2000x2000 to ensure all plots match the desired resolution\n",
    "    for ax in axes:\n",
    "        ax.set_xlim([0, 2000])\n",
    "        ax.set_ylim([0, 2000])\n",
    "        ax.invert_yaxis()  # Fix the inverted y-axis for correct orientation\n",
    "\n",
    "    # 1. Plot C2-C7 Angle\n",
    "    ax = axes[0]\n",
    "    ax.imshow(test_image, cmap='gray', origin='upper')  # Show the actual test image\n",
    "\n",
    "    # Overlay the predicted binary mask (class 1 in cyan)\n",
    "    ax.imshow(binary_mask, cmap='cividis', alpha=0.5)  # Use 'cividis' colormap to show class 1 in cyan\n",
    "\n",
    "    # Draw lines through the left and right corners for C2 and C7\n",
    "    ax.plot([c2_left_corner[0], c2_right_corner[0]], [c2_left_corner[1], c2_right_corner[1]], 'lime', label=\"C2 Border\")\n",
    "    ax.plot([c7_left_corner[0], c7_right_corner[0]], [c7_left_corner[1], c7_right_corner[1]], 'lime', label=\"C7 Border\")\n",
    "\n",
    "    # Define the range for the perpendicular lines starting from the posterior corners\n",
    "    c2_dx, c2_dy = 250, 250 * c2_perpendicular_slope  \n",
    "    c7_dx, c7_dy = 250, 250 * c7_perpendicular_slope  \n",
    "\n",
    "    # Draw the perpendicular lines passing through the posterior corners\n",
    "    ax.plot([c2_left_corner[0] + c2_dx, c2_left_corner[0] - c2_dx], \n",
    "            [c2_left_corner[1] + c2_dy, c2_left_corner[1] - c2_dy], 'lime')\n",
    "    ax.plot([c7_left_corner[0] - c7_dx, c7_left_corner[0] + c7_dx], \n",
    "            [c7_left_corner[1] - c7_dy, c7_left_corner[1] + c7_dy], 'lime')\n",
    "\n",
    "    # Add text annotation for C2-C7 Angle next to the contour\n",
    "    ax.text(c2_right_corner[0] + 10, c2_right_corner[1], f\"C2-C7 Angle: {pred_results['c2_c7_angle']:.2f}°\", color='lime', fontsize=12)\n",
    "\n",
    "    ax.set_title(\"C2-C7 Angle\")\n",
    "\n",
    "    # 2. Plot CGH-C7 SVA\n",
    "    ax = axes[1]\n",
    "    ax.imshow(test_image, cmap='gray', origin='upper')  # Show the actual test image\n",
    "    \n",
    "    # Overlay the predicted binary mask (class 1 in cyan)\n",
    "    ax.imshow(binary_mask, cmap='cividis', alpha=0.5)  # Use 'cividis' colormap to show class 1 in cyan\n",
    "    \n",
    "    acoustic_meatus_anterior = tuple(contours_acoustic_meatus[0][contours_acoustic_meatus[0][:, :, 0].argmax()][0])\n",
    "    c7_posterior_corner = c7_left_corner\n",
    "    \n",
    "    ax.plot([acoustic_meatus_anterior[0], acoustic_meatus_anterior[0]], [acoustic_meatus_anterior[1], c7_posterior_corner[1]], 'lime')\n",
    "    ax.plot([acoustic_meatus_anterior[0], c7_posterior_corner[0]], [c7_posterior_corner[1], c7_posterior_corner[1]], 'r-', label=\"CGH-C7 SVA\")\n",
    "\n",
    "    # Add text annotation for CGH-C7 SVA next to the contour\n",
    "    ax.text(acoustic_meatus_anterior[0] + 10, acoustic_meatus_anterior[1], f\"CGH-C7 SVA: {pred_results['cgh_c7_sva']:.2f} mm\", color='lime', fontsize=12)\n",
    "\n",
    "    ax.set_title(\"CGH-C7 SVA\")\n",
    "\n",
    "    # 3. Plot C2 and C7 Slopes\n",
    "    ax = axes[2]\n",
    "    ax.imshow(test_image, cmap='gray', origin='upper')  # Show the actual test image\n",
    "    \n",
    "    # Overlay the predicted binary mask (class 1 in cyan)\n",
    "    ax.imshow(binary_mask, cmap='cividis', alpha=0.5)  # Use 'cividis' colormap to show class 1 in cyan\n",
    "    \n",
    "    c2_lower_border = tuple(contours_c2[0][contours_c2[0][:, :, 1].argmax()][0])\n",
    "    c7_lower_border = tuple(contours_c7[0][contours_c7[0][:, :, 1].argmax()][0])\n",
    "\n",
    "    # Draw horizontal lines for C2 and C7 slopes\n",
    "    ax.plot([c2_lower_border[0] - 300, c2_lower_border[0] + 100], [c2_lower_border[1], c2_lower_border[1]], 'lime')\n",
    "    ax.plot([c7_lower_border[0] - 300, c7_lower_border[0] + 100], [c7_lower_border[1], c7_lower_border[1]], 'lime')\n",
    "    \n",
    "    # Adjust the slope line for C2 and C7\n",
    "    ax.plot([c2_left_corner[0], c2_right_corner[0]], [c2_left_corner[1], c2_right_corner[1]], 'lime', label=\"C2 Border\")\n",
    "    ax.plot([c7_left_corner[0], c7_right_corner[0]], [c7_left_corner[1], c7_right_corner[1]], 'lime', label=\"C7 Border\")\n",
    "\n",
    "    # Add text annotations for C2 and C7 slopes next to the contour\n",
    "    ax.text(c2_left_corner[0] + 10, c2_left_corner[1], f\"C2 Slope: {pred_results['c2_slope']:.2f}°\", color='lime', fontsize=12)\n",
    "    ax.text(c7_left_corner[0] + 10, c7_left_corner[1], f\"C7 Slope: {pred_results['c7_slope']:.2f}°\", color='lime', fontsize=12)\n",
    "\n",
    "    ax.set_title(\"C2 and C7 Slopes\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Predict on test data and visualize for all files\n",
    "img_files = sorted(glob.glob(test_img_dir))  # Get all image file paths in sorted order\n",
    "\n",
    "# Loop over all the files in the test dataset\n",
    "for idx, (test_img_batch, test_mask_batch) in enumerate(test_dataset):\n",
    "    if idx >= num_test_samples:  # Stop once we've processed all samples\n",
    "        break\n",
    "\n",
    "    y_pred = model.predict(test_img_batch)  # Model prediction on the batch\n",
    "\n",
    "    # Convert predictions to binary format (threshold at 0.5)\n",
    "    y_pred_binary = (y_pred > 0.5).astype(np.uint8)\n",
    "\n",
    "    # Get the file name for the current image\n",
    "    file_name = img_files[idx].split(\"/\")[-1]  # Extract file name from path\n",
    "    print(f\"Processing file: {file_name}\")\n",
    "\n",
    "    # Loop over the batch (only 1 image in this case)\n",
    "    for i in range(test_img_batch.shape[0]):\n",
    "        test_image = np.squeeze(test_img_batch[i])  # Get the test image\n",
    "        pred_mask = np.squeeze(y_pred_binary[i])  # Get the predicted mask\n",
    "\n",
    "        pred_mask_processed = preprocess_prediction(pred_mask)  # Preprocess prediction\n",
    "\n",
    "        # If the processed mask is None (i.e., less than 3 clusters), skip the current iteration\n",
    "        if pred_mask_processed is None:\n",
    "            continue\n",
    "\n",
    "        # Calculate measurements and angles for the prediction\n",
    "        pred_results, contours_acoustic_meatus, contours_c2, contours_c7, c2_left_corner, c2_right_corner, c7_left_corner, c7_right_corner, c2_perpendicular_slope, c7_perpendicular_slope = calculate_distances_and_angles(pred_mask_processed)\n",
    "\n",
    "        # Plot all the measurements with annotations in their respective subplots\n",
    "        plot_all(test_image, pred_mask_processed, contours_acoustic_meatus, contours_c2, contours_c7, \n",
    "                 c2_left_corner, c2_right_corner, c7_left_corner, c7_right_corner, \n",
    "                 c2_perpendicular_slope, c7_perpendicular_slope, pred_results)\n",
    "\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
